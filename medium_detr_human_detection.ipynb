{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "medium_detr_human_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLCPOqaGnnIP1IV+UP3/SU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MastafaF/DETR/blob/main/medium_detr_human_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXEQ9VJbFh3k"
      },
      "source": [
        "<table align=\"center\"><td>\n",
        "  <a target=\"_blank\"  href=\"https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb\">\n",
        "    <img src=\"https://cdn.mos.cms.futurecdn.net/xJGh6cXvC69an86AdrLD98-320-80.jpg\" />View Medium article\n",
        "  </a>\n",
        "</td><td>\n",
        "</td></table>\n",
        "\n",
        "This notebook contains software developed by Ultralytics LLC, and **is freely available for redistribution under the GPL-3.0 license**. For more information please visit https://github.com/ultralytics/yolov3 and https://www.ultralytics.com.\n",
        "\n",
        "This notebook has been developped and adapted by Mastafa Foufa. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5AodUFIEN3c"
      },
      "source": [
        "\"\"\"\n",
        "Building a simple dataset from camera footage\n",
        "\"\"\"\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "URL = 'http://83.140.123.184/ImageHarvester/Images/copyright!-stureplan_1_live.jpg'\n",
        "\n",
        "img_arr = []\n",
        "for idx_image in range(1, 20):\n",
        "  response = requests.get(url=URL, stream=True)\n",
        "  # GET IMAGE IN REQUEST\n",
        "  img = Image.open(response.raw)\n",
        "  img_arr.append(img)\n",
        "\n",
        "for idx, img in enumerate(img_arr): \n",
        "  img.save(\"data/samples/img_stockholm_\"+str(idx)+\".jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az7ViA86Hwxj"
      },
      "source": [
        "#@title Object Detection with DETR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iH72xkYDlqc"
      },
      "source": [
        "\"\"\"\n",
        "Object detection with DETR \n",
        "\"\"\"\n",
        "\n",
        "import torch as th \n",
        "import torchvision.transforms as T \n",
        "import requests \n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# Instantiate DETR model \n",
        "model = th.hub.load('facebookresearch/detr', 'detr_resnet101', pretrained=True)\n",
        "model.eval()\n",
        "model = model.cuda()\n",
        "\n",
        "# Define classes to be detected and transformations of our original images \n",
        "# standard PyTorch mean-std input image normalization\n",
        "# T.Nomarlize([mean_R, mean_G, mean_B], [std_R, std_G, std_B])\n",
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "CLASSES = [\n",
        "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
        "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
        "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
        "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
        "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
        "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
        "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
        "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "    'toothbrush'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySCdSbMrDzy2"
      },
      "source": [
        "\"\"\"\n",
        "Simple detection of people:\n",
        "Example following https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb#scrollTo=Y6Jrz6xz71C0 \n",
        "\"\"\"\n",
        "\n",
        "import torch \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# colors for visualization\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "\n",
        "# standard PyTorch mean-std input image normalization\n",
        "transform = T.Compose([\n",
        "    T.Resize(800),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# for output bounding box post-processing\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    # Push to CPU to perform operation after\n",
        "    b = box_cxcywh_to_xyxy(out_bbox).cpu()\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b\n",
        "\n",
        "def detect(im, model, transform, confidence_level = 0.7):\n",
        "    # mean-std normalize the input image (batch-size: 1)\n",
        "    img = transform(im).unsqueeze(0)\n",
        "    # Additional if on GPU: push image to GPU\n",
        "    img = img.cuda()\n",
        "    # propagate through the model\n",
        "    outputs = model(img)\n",
        "\n",
        "    # keep only predictions with 0.7+ confidence\n",
        "    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "    keep = probas.max(-1).values > confidence_level # CHANGED FOR MORE PREDICTIONS\n",
        "\n",
        "    # convert boxes from [0; 1] to image scales\n",
        "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
        "    return probas[keep], bboxes_scaled\n",
        "\n",
        "\n",
        "\n",
        "# url_classroom = \"https://docs.openvinotoolkit.org/2020.2/person-detection-action-recognition-teacher-0002.png\"\n",
        "# url_camera_footage = 'http://data.goteborg.se/TrafficCamera/v0.2/CameraImage/83a5905d-7889-4bae-bd6d-0a7e67f4f6af/16'\n",
        "# im = Image.open(requests.get(url_camera_footage, stream=True).raw).resize((800,600)).convert('RGB')\n",
        "\n",
        "\"\"\"\n",
        "One can define an array of images img_arr where we can store images \n",
        "\"\"\"\n",
        "# In the array img_arr, we have previously stored images from camera footage \n",
        "for idx, im in enumerate(img_arr): \n",
        "  scores, boxes = detect(im, model, transform)\n",
        "\n",
        "  def plot_results(pil_img, prob, boxes, filename):\n",
        "      plt.figure(figsize=(16,10))\n",
        "      plt.imshow(pil_img)\n",
        "      ax = plt.gca()\n",
        "      COUNT_PERSON = 0\n",
        "      for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n",
        "          \n",
        "          cl = p.argmax()\n",
        "          label = CLASSES[cl]\n",
        "          if label == \"person\": \n",
        "            COUNT_PERSON += 1 \n",
        "            text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
        "\n",
        "            ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                      fill=False, color=c, linewidth=3))\n",
        "            \n",
        "            ax.text(xmin, ymin, text, fontsize=15,\n",
        "                    bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "            \n",
        "      print(\"Number of people detected: {}\".format(COUNT_PERSON))\n",
        "      plt.axis('off')\n",
        "      # plt.show()\n",
        "      plt.savefig(filename)\n",
        "      return\n",
        "  # Saving images with 'human detection' in the folder detr_output\n",
        "  plot_results(im, scores, boxes, filename = \"detr_output/img_stockholm_detr_detection\" + str(idx) + \".jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcr0R2oVHqm_"
      },
      "source": [
        "#@title DETR vs YOLO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDvwDo97EV4B"
      },
      "source": [
        "##### Yolo ###### \n",
        "\n",
        "\"\"\"\n",
        "Yolo implementation from https://github.com/ultralytics/yolov3 \n",
        "\"\"\"\n",
        "import time\n",
        "import glob\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from IPython.display import Image, clear_output \n",
        "print('PyTorch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZQjeC0pEZ-1"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov3  # clone\n",
        "!bash yolov3/data/get_coco_dataset_gdrive.sh  # copy COCO2014 dataset (19GB)\n",
        "%cd yolov3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W40KmtjEfFP"
      },
      "source": [
        "\"\"\"\n",
        "A simple way to apply the Pytorch implementation of Yolo on your images \n",
        "is to add your own images in the folder data/samples.\n",
        "\"\"\"\n",
        "\n",
        "!python3 detect.py\n",
        "# The original image of Zidane is in the folder data/samples \n",
        "# The image with detected objects is in the folder output\n",
        "Image(filename='output/zidane.jpg', width=600)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}